# Module-3-A3C-LSTM
Impl√©mentation A3C + LSTM

## üéÆ Introduction

Dans ce module, nous impl√©mentons une combinaison de techniques avanc√©es en apprentissage par renforcement pour entra√Æner un agent √† r√©soudre des t√¢ches complexes. Cette impl√©mentation repose sur deux concepts cl√©s : **A3C** (Actor-Critic Asynchronous Advantage) et **LSTM** (Long Short Term Memory).

- **A3C** : Actor-Critic Asynchronous Advantage
- **LSTM** : Long Short Term Memory

---

## üß† A3C : Actor-Critic, Asynchronous, Advantage

### 1. Actor-Critic

<img src="Actor.png" alt="Texte alternatif" width="250"/>

L'architecture **Actor-Critic** repose sur deux parties principales :

- **L'acteur (Actor)** : Cette partie prend une d√©cision en choisissant l'action √† effectuer dans un √©tat donn√©. Elle utilise une fonction de politique pour g√©n√©rer un ensemble d'actions.
- **Le critique (Critic)** : Cette partie √©value la qualit√© de l'action choisie par l'acteur en estimant la valeur de l'√©tat dans lequel se trouve l'agent.

Les valeurs **Q** de l‚Äôacteur sont appel√©es strat√©gie et sont parfois not√©es par la lettre grecque **œÄ**. Ces valeurs sont utilis√©es pour guider l‚Äôacteur dans ses choix d‚Äôactions.

### 2. Asynchronous

Le concept **Asynchronous** (Asynchrone) permet √† plusieurs agents d'interagir avec l'environnement en m√™me temps. Chaque agent commence √† un point diff√©rent de l'environnement et apporte ses connaissances aux autres agents via le r√©seau partag√©. Cette approche am√©liore l'exploration et r√©duit le risque d'overfitting ou de stagnation dans certaines parties de l'environnement.

<img src="A3C-reinforcement-learning.jpg" alt="Texte alternatif" width="250"/>

Les agents partagent un r√©seau neuronal, qui g√©n√®re des politiques (acteurs) et des critiques. En cas de mise √† jour des poids du r√©seau, tous les agents partagent les m√™mes poids, mais leurs actions sont distinctes et bas√©es sur les contextes sp√©cifiques de chaque agent.

### 3. Advantage

L‚Äô**avantage** (Advantage) repr√©sente la diff√©rence entre la **valeur de l‚Äô√©tat** (estim√©e par le critique) et la **valeur d‚Äôaction** choisie. Cet avantage est utilis√© pour ajuster les d√©cisions de l‚Äôacteur et am√©liorer ses actions futures. 

L‚Äôavantage est calcul√© avec l‚Äô√©quation suivante :
\[ \text{Advantage}(s, a) = Q(s, a) - V(s) \]
L‚Äôobjectif est de maximiser l‚Äôavantage pour apprendre plus efficacement les bonnes actions.

La **perte de police** est utilis√©e pour inciter l'agent √† choisir des actions positives, tandis que la **perte de valeur** est li√©e √† l‚Äô√©valuation de l‚Äô√©tat (valeur de l‚Äô√©tat).

### 4. LSTM : Long Short Term Memory

<img src="LSTM.png" alt="Texte alternatif" width="250"/>


Le **LSTM** est une couche sp√©cialis√©e dans les r√©seaux de neurones r√©currents (RNN). Contrairement aux r√©seaux classiques, les LSTM poss√®dent une m√©moire qui leur permet de se souvenir des actions pass√©es et d'en tenir compte pour les d√©cisions futures. Cela est particuli√®rement utile pour les environnements o√π la dynamique des √©tats √©volue au fil du temps, comme les jeux vid√©o.

Dans cette impl√©mentation, la couche **LSTMCell** est utilis√©e pour stocker les informations temporelles des observations successives. Elle remplace la couche cach√©e classique des r√©seaux de neurones pour capturer les d√©pendances temporelles et am√©liorer la performance de l‚Äôagent sur des t√¢ches complexes.

---

## üèóÔ∏è La Classe `ActorCritic`

La classe `ActorCritic` d√©finit un r√©seau de neurones qui impl√©mente la m√©thode **Actor-Critic** pour l'apprentissage par renforcement avec des observations sous forme d‚Äôimages.

### Structure du mod√®le

Le mod√®le est compos√© de plusieurs couches :

- **4 couches de convolution (`conv1`, `conv2`, `conv3`, `conv4`)** : Ces couches sont responsables de l'extraction des caract√©ristiques visuelles de l‚Äôimage d'entr√©e.
- **LSTMCell** : Une cellule LSTM qui capture les d√©pendances temporelles dans les donn√©es s√©quentielles.
- **Couches lin√©aires (`critic_linear` et `actor_linear`)** : 
  - `critic_linear` : Estime la valeur d‚Äô√©tat.
  - `actor_linear` : G√©n√®re la distribution de probabilit√©s pour les actions possibles.

### Initialisation des poids

Les poids sont initialis√©s pour stabiliser l'apprentissage :
- Les poids du **Critic** sont initialis√©s avec une variance plus large.
- Les poids de l'**Actor** sont initialis√©s avec des valeurs plus petites pour √©viter des mises √† jour brutales.
- Les biais de la LSTM et des couches lin√©aires sont initialis√©s √† z√©ro.

### Fonction `forward`

La m√©thode `forward` d√©finit le passage des donn√©es dans le r√©seau. Elle prend en entr√©e une observation sous forme de tenseur, ainsi que les √©tats internes de la LSTM (hx, cx). Le flux des donn√©es se fait comme suit :

1. Passage des donn√©es d'entr√©e √† travers les couches de convolution activ√©es par la fonction **ELU**.
2. Les caract√©ristiques extraites sont envoy√©es dans la cellule LSTM pour capturer les relations temporelles.
3. La sortie de la LSTM est envoy√©e √† travers les couches lin√©aires pour obtenir les pr√©dictions :
   - **Critique** : Estime la valeur de l‚Äô√©tat courant.
   - **Acteur** : G√©n√®re une distribution de probabilit√©s pour les actions possibles.

### Code de la classe `ActorCritic`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# Initialisation des poids pour stabiliser l'apprentissage
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
    elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
    elif isinstance(m, nn.LSTMCell):
        nn.init.zeros_(m.bias_ih)
        nn.init.zeros_(m.bias_hh)

def normalized_columns_initializer(weights, std=1.0):
    shape = weights.shape
    out = torch.randn(shape) * std
    return out

class ActorCritic(torch.nn.Module):
    def __init__(self, num_inputs, action_space):
        super(ActorCritic, self).__init__()
        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
        self.lstm = nn.LSTMCell(32 * 3 * 3, 256)
        num_outputs = action_space.n
        self.critic_linear = nn.Linear(256, 1)
        self.actor_linear = nn.Linear(256, num_outputs)
        self.apply(weights_init)
        self.actor_linear.weight.data = normalized_columns_initializer(self.actor_linear.weight.data, 0.01)
        self.actor_linear.bias.data.fill_(0)
        self.critic_linear.weight.data = normalized_columns_initializer(self.critic_linear.weight.data, 1.0)
        self.critic_linear.bias.data.fill_(0)
        self.lstm.bias_ih.data.fill_(0)
        self.lstm.bias_hh.data.fill_(0)
        self.train()

    def forward(self, inputs):
        inputs, (hx, cx) = inputs
        x = F.elu(self.conv1(inputs))
        x = F.elu(self.conv2(x))
        x = F.elu(self.conv3(x))
        x = F.elu(self.conv4(x))
        x = x.view(-1, 32 * 3 * 3)
        hx, cx = self.lstm(x, (hx, cx))
        x = hx
        return self.critic_linear(x), self.actor_linear(x), (hx, cx)
```

## üèÉ‚Äç‚ôÇÔ∏è Entra√Ænement et Test

### 1. Entra√Ænement avec Parall√©lisme

L'impl√©mentation A3C permet d'utiliser plusieurs agents en parall√®le pour am√©liorer l'exploration et acc√©l√©rer l'entra√Ænement. Chaque agent collecte ses propres exp√©riences et met √† jour le r√©seau de mani√®re asynchrone.

- **Classe Test** : Effectue des tests p√©riodiques pour √©valuer la performance de l'agent.
- **Classe Train** : Effectue l'entra√Ænement du mod√®le, en effectuant la mise √† jour des poids du r√©seau √† chaque √©pisode.

### 2. Code d'Entra√Ænement

Le code d‚Äôentra√Ænement doit inclure des agents multiples, un r√©seau partag√© et un critique global. Les agents travaillent ind√©pendamment mais partagent les poids du mod√®le pour permettre l‚Äôapprentissage global.

### üìö Ressources

- PyTorch Documentation
- A3C Paper
- LSTM Overview

Bonne chance avec l‚Äôimpl√©mentation A3C et LSTM ! üéÆ
